{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPfDsdRPm9MlTEUmNY4GpI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Jz5QOMTgGa",
        "outputId": "bd3455ae-91b6-4185-8c58-fd986ee0ab27",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_google_genai langchain_core langchain_community langgraph langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "7ib0VghbT9pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"Difference betwen langgraph and crewai?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zMStN8EUCOS",
        "outputId": "74446303-038b-4704-8d47-de9d28b7a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"LangChain and CrewAI are both platforms that leverage large language models (LLMs), but they cater to different needs and have distinct approaches:\\n\\n**LangChain:**\\n\\n* **Focus:**  Building applications *with* LLMs.  It's a framework that provides modular components and tools to integrate LLMs into your own custom applications.  Think of it as a toolbox for developers.\\n* **Functionality:** Offers components for chaining multiple LLMs together, connecting to external data sources (databases, APIs), managing memory, and handling various LLM interactions.  It's highly customizable and allows for complex workflows.\\n* **Abstraction Level:** Low-level. You're writing code to integrate and manage the LLMs.\\n* **Use Cases:** Building custom applications like chatbots, question-answering systems, document summarizers, and more.  It's ideal for developers who want fine-grained control over the LLM interaction.\\n* **Ease of Use:** Requires programming skills.  It's not a no-code/low-code platform.\\n\\n\\n**CrewAI:**\\n\\n* **Focus:**  Building and deploying *LLM-powered applications* more easily. It aims to simplify the process of creating and managing applications.  Think of it as a platform that handles much of the underlying infrastructure.\\n* **Functionality:** Provides a platform for building and deploying applications, often with a focus on specific use cases like chatbots or document processing.  It may offer pre-built templates and integrations.\\n* **Abstraction Level:** Higher-level than LangChain.  It abstracts away much of the complexity of LLM interaction and infrastructure management.\\n* **Use Cases:** Rapid prototyping and deployment of LLM-based applications.  It's suitable for developers and non-developers who want a quicker path to deployment.\\n* **Ease of Use:** Generally easier to use than LangChain, potentially offering no-code or low-code options depending on the specific features.\\n\\n\\n**In short:**\\n\\n* Choose **LangChain** if you need fine-grained control, complex workflows, and are comfortable with programming.  You're building the application from the ground up, using LangChain as a powerful toolkit.\\n* Choose **CrewAI** (or similar platforms) if you want a faster path to deployment, simpler workflows, and potentially less coding.  You're leveraging a platform that handles much of the heavy lifting.\\n\\n\\nIt's not an either/or situation.  You might even use LangChain to build a custom component that then integrates into a CrewAI application.  The best choice depends on your technical skills, project requirements, and desired level of control.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-2f207219-bdaf-490c-ac14-ec9d720ae6ed-0' usage_metadata={'input_tokens': 9, 'output_tokens': 553, 'total_tokens': 562, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# humanmessage way of talking with chatmodel\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "msg = [\n",
        "    HumanMessage(content=\"Difference betwen langgraph and crewai in 20 words only?\",name=\"Teacher\"),\n",
        "    AIMessage(content=\"\"\"LangChain focuses on application building;CrewAI emphasizes large language model (LLM) training and improvement.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-440a6c3c-4faa-4a23-939f-157cbb211724-0' usage_metadata={'input_tokens': 15, 'output_tokens': 23, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}}\"\"\",name=\"AI\"),\n",
        "    HumanMessage(content=\"explain more\",name=\"Teacher\")\n",
        "]\n",
        "\n",
        "llm.invoke(msg)"
      ],
      "metadata": {
        "id": "RYHfJd1MVEQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6635d9-2462-4d6b-a9e6-345cb9b1131f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"LangChain is a framework for building applications *using* large language models (LLMs).  Think of it as the Lego bricks and instructions for assembling an LLM-powered application, like a chatbot or a document question-answering system.  You integrate pre-trained LLMs and add functionalities like memory and external data access.\\n\\nCrewAI, on the other hand, focuses on *improving* LLMs themselves.  They work on training better models, often specializing in specific domains or tasks.  They're more concerned with the underlying model's performance and capabilities than with building end-user applications directly.  They might create a new, more accurate LLM for medical diagnosis, for example, rather than building a doctor's appointment scheduling app.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-3b348219-d3f4-4c92-9b9e-352134f4fc7f-0', usage_metadata={'input_tokens': 164, 'output_tokens': 155, 'total_tokens': 319, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['TAVILY_API_KEY'] = userdata.get('TAVILY_API_KEY')\n",
        "# Get the Tavily API key from user data.\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# Initialize the TavilySearchResults tool with the API key and desired number of results.\n",
        "tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=3)\n",
        "\n",
        "# Use the tool to search for \"what is langchain?\".\n",
        "results = tavily_search.invoke(\"what is langchain?\")\n",
        "\n",
        "# Print the search results.\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pi7eprYjQHm-",
        "outputId": "1e071b40-9b89-4c87-9dd4-e0b5fe7e9dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'url': 'https://www.techtarget.com/searchenterpriseai/definition/LangChain', 'content': 'LangChain is an open source framework that enables software developers working with artificial intelligence (AI) and its machine learning subset to combine large language models with other external components to develop LLM-powered applications. LangChain is a framework that simplifies the process of creating generative AI application interfaces. LangChain includes prompt template modules that enable developers to create structured prompts for LLMs. These templates can incorporate examples and specify output formats, facilitating smoother interactions and more accurate responses from the models. The LangChain framework helps developers create LLM-powered applications by offering tools that build intricate workflows and integrate different components. Use LangChain and vector search on Amazon DocumentDB to build a generative AI ...'}, {'url': 'https://python.langchain.com/v0.1/docs/get_started/introduction/', 'content': \"LangChain is a framework for developing applications powered by large language models (LLMs). Development: Build your applications using LangChain's open-source building blocks and components. langchain-community: Third party integrations. langserve: Deploy LangChain chains as REST APIs. LangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications and seamlessly integrates with LangChain. We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application. LangChain Expression Language (LCEL) is the foundation of many of LangChain's components, and is a declarative way to compose chains. LangChain provides standard, extendable interfaces and integrations for many different components, including: LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\"}, {'url': 'https://aws.amazon.com/what-is/langchain/', 'content': 'LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. LangChain provides AI developers with tools to connect language models with external data sources. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. What Is AWS? Developers on AWS'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YVyY-2RoR-VQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}